\section{Question 1}

The data are shown on \autoref{q1_two_observations}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/q1.pdf}
  \caption{The input data showing two types of observations with circles and triangles. Plotting code: \url{https://github.com/evgenyneu/ASP5020_data_analysis_problem_sets/blob/master/ps5/code/q1.py}.}
  \label{q1_two_observations}
\end{figure}

The diagram of the neural network is shown on \autoref{q1_network_diagram}. I choose the sigmoid activation function for the hidden layer nodes, because it's a common one in use. I will see how it work and switch to \emph{tanh} or \emph{ReLU} if needed. This is a classification task with just a single binary output (0 or 1). Therefore, for simplicity, I chose a single output node with no activation function (i.e. $f(x) = x$). If there were more than one output nodes than I could try a softmax function, but I will start simple and see how it goes.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/q1_neural_network.pdf}
  \caption{Diagram of the neural network containing two inputs ($x_1$ and $x_2$), single hidden layer with three nodes ($h_1$, $h_2$ and $h_3$) and a single output layer ($o_1$). The red circles with $1$ correspond to biases. Hidden layer nodes use sigmoid activation function. The labels $w1$ through $w13$ corredpond to the weight values, which are initialized by drawing random numbers from normal distribution $N(0, 1)$. Image source: \url{https://github.com/evgenyneu/ASP5020_data_analysis_problem_sets/blob/master/ps5/report/figures/q1_neural_network.sketch}}
  \label{q1_network_diagram}
\end{figure}


\subsection{Why I don't use sigmoid in the output layer?}

I do not use the sigmoid activation function in the output neuron because of the problem I found in the the Lecture 13 example code (\url{http://astrowizici.st/teaching/phs5000/13/}). I modified the code and replaced the sigmoid with $f(x) = x$ function. This was done because the output of the sigmoid is between 0 and 1, and it is used as predicted value from our model. However, in the loss function, we are comparing the predicted values with real values, which fall outside the $[0, 1]$ range. Therefore, the predicted value can never, even in theory, match the data. This mismatch is shown on \autoref{q1_lecture13_loss_compare}, where I compare the loss function of the original and the modified models. We can see that the modified model converges faster. The loss function for the original model will never approach zero, even if we increase the number of interations because of the range difference between the sigmoid and the data.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{figures/lecture13_loss_compared.pdf}
  \caption{Loss function for neural network from Lecture 13. The original code (left) contains sigmoid activation function in the output node, while the modified code (right) has no activation function ($f(x) = x$). Both codes are otherwise identical, including the same seeding of random number generators. The loss function of the original code (left) converges more slowly and never approaches zero. This is caused by mismatch between the range $[0, 1]$ of the sigmoid function and the data values, which are normalized to zero mean and standard deviation of 1.}
  \label{q1_lecture13_loss_compare}
\end{figure}

The comparison of real and predicted values for the two codes are shown on \autoref{q1_lecture13_data_prediction_compare}. We can see that the original model can not be used for generating predicted values, since its range is limited.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\textwidth]{figures/lecture13_results_compared.pdf}
  \caption{Comparison of model prediction (y axis) with data (x axis) for neural network code from Lecture 13. The original code (left) performs worse because it's output range is limited to $[0, 1]$ multiplied by standard deviation and added mean of the data.}
  \label{q1_lecture13_data_prediction_compare}
\end{figure}



Original code form the lecture notes: \\ \url{https://github.com/evgenyneu/ASP5020_data_analysis_problem_sets/blob/master/ps5/code/lecture13_original_with_signoid_output.py}

My modified code without the output sigmoid (changed lines 99, 140 and 153): \\ \url{https://github.com/evgenyneu/ASP5020_data_analysis_problem_sets/blob/master/ps5/code/lecture13_remove_sigmoid_from_output.py}

The code changes are shown on \autoref{q1_lecture13_removing_loss_function}.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/q1_lecture13_code_changes.png}
  \caption{Code changes that show removal of the sigmoid activation function from the output node.}
  \label{q1_lecture13_removing_loss_function}
\end{figure}



\subsection{Equations for the neuron inputs and outpus}

I will use $a$ and $b$ for input and output value of a neuron respectively, so $a_{h2}$, for example is the input value of the $h_2$ neuron, and $b_{h2}$ is it's output value. Then, for the hidden layer the inputs are
\begin{align}
  \label{eq_q1_hidden_neurons_inputs}
  \begin{split}
    a_{h1} &= w_1 + x_1 w_4 + x_2 w_7 \\
    a_{h2} &= w_2 + x_1 w_5 + x_2 w_8 \\
    a_{h3} &= w_3 + x_1 w_6 + x_2 w_9.
  \end{split}
\end{align}
The outputs are
\begin{align}
  \label{eq_q1_hidden_neurons_outputs}
  \begin{split}
    b_{h1} &= \text{sigmoid}(a_{h1}) \\
    b_{h2} &= \text{sigmoid}(a_{h2}) \\
    b_{h3} &= \text{sigmoid}(a_{h3}),
  \end{split}
\end{align}
where $\text{sigmoid}$ is my chosen activation function
\[
\text{sigmoid}(x) = \frac{1}{1 + e^{-x}}.
\]
The input and output of the output node are the same, since I chose not to use an activation function:
\begin{equation}
  a_{o1} = w_{10} + b_{h1} w_{11} + b_{h2} w_{12} + b_{h3} w_{13}.
  \label{eq_q1_output_value}
\end{equation}


\subsection{The loss function}

The loss function is
\begin{align}
  E = 0.5 (y - a_{o1})^2,
  \label{eq_q1_loss_function}
\end{align}
where $y$ is the observed binary value (0 or 1).


\subsection{Derivatives of the loss function}

Next, I calculate the derivatives of the loss function with respect to all the weights. For the weight $w$ coming from the hidden layer, the derivatives are calculated using equation
\begin{align*}
  \frac{\partial E}{\partial w} = \frac{\partial E}{\partial a_o} \frac{\partial a_o} {\partial w}.
\end{align*}
Next, I substitute $E$ and $a_o$ from Equations \ref{eq_q1_output_value} and \ref{eq_q1_loss_function} and calculate the derivatives for all the hidden layer weights:
\begin{align*}
  \frac{\partial E}{\partial w_{10}} &= a_{o1} - y \\
  \frac{\partial E}{\partial w_{11}} &= (a_{o1} - y) b_{h1} \\
  \frac{\partial E}{\partial w_{12}} &= (a_{o1} - y) b_{h2} \\
  \frac{\partial E}{\partial w_{13}} &= (a_{o1} - y) b_{h3}.
\end{align*}
For a weight coming form the input layer, the derivative is calculated with equation
\begin{align*}
  \frac{\partial E}{\partial w} = \frac{\partial E}{\partial a_o} \frac{\partial a_o} {\partial b_h} \frac{\partial b_h} {\partial a_h} \frac{\partial a_h} {\partial w}.
\end{align*}
Here I use \autoref{eq_q1_hidden_neurons_outputs} for $b_h$ and \autoref{eq_q1_hidden_neurons_inputs} for $a_h$ and calculate the derivatives for all the input weights:
\begin{align*}
  \frac{\partial E}{\partial w_{1}} &= (a_{o1} - y) w_{11} b_{h1} (1 - b_{h1}) \\
  \frac{\partial E}{\partial w_{2}} &= (a_{o1} - y) w_{12} b_{h2} (1 - b_{h2}) \\
  \frac{\partial E}{\partial w_{3}} &= (a_{o1} - y) w_{13} b_{h3} (1 - b_{h3}) \\
  \frac{\partial E}{\partial w_{4}} &= (a_{o1} - y) w_{11} b_{h1} (1 - b_{h1}) x_1 \\
  \frac{\partial E}{\partial w_{5}} &= (a_{o1} - y) w_{12} b_{h2} (1 - b_{h2}) x_1 \\
  \frac{\partial E}{\partial w_{6}} &= (a_{o1} - y) w_{13} b_{h3} (1 - b_{h3}) x_1 \\
  \frac{\partial E}{\partial w_{7}} &= (a_{o1} - y) w_{11} b_{h1} (1 - b_{h1}) x_2 \\
  \frac{\partial E}{\partial w_{8}} &= (a_{o1} - y) w_{12} b_{h2} (1 - b_{h2}) x_2 \\
  \frac{\partial E}{\partial w_{9}} &= (a_{o1} - y) w_{13} b_{h3} (1 - b_{h3}) x_2.
\end{align*}

\subsection{Updating the weights}

The equation for updating a weight is
\[
  w^{i+1} = w^{i} - e \sum_{j=1}^{N} \frac{\partial E(y_j)}{\partial w},
\]
where $w^{i+1}$ and $w^{i}$ are the weight values for the current and the previous iterations respectively, $e$ is a learning rate (a small number like $0.001$), $N$ is the number of observations and $y_j$ is the $j-th$ observation.
